{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Assignment 2\n",
    "\n",
    "**By Genesis Qu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: True and False\n",
    "\n",
    "**Problem 1.1**\n",
    "\n",
    "True. Batch Normalization scales the output by the mean and variance.\n",
    "\n",
    "**Problem 1.2**\n",
    "\n",
    "True. Yes, you can use PyTorch's pre-defined optimizers to do back-propagation.\n",
    "\n",
    "**Problem 1.3**\n",
    "\n",
    "False. Not every data augmentation technique will be applicable to our image. Vertical flip, for example, will not be applicable to the CIFAR-10 dataset because a digit such as 6 will be recognized as 9 by the network.\n",
    "\n",
    "**Problem 1.4**\n",
    "\n",
    "False. Dropout and batch normalization do not make CNNs convert faster. They make convergence more stable and also limits overfitting to the training data.\n",
    "\n",
    "**Problem 1.5**\n",
    "\n",
    "False. Sometimes dropout layers do not perform well in cooperation with L-norm regularizations.\n",
    "\n",
    "**Problem 1.6**\n",
    "\n",
    "True. Because the L1 regularization space has a diamond contour, it will often meet the solution loss function at its corner, thus obtaining sparser weights.\n",
    "\n",
    "**Problem 1.7**\n",
    "\n",
    "True. Although Leaky ReLU solves the dead neuron issue, the inconsistent slope of the activation makes some training processes unstable.\n",
    "\n",
    "**Problem 1.8**\n",
    "\n",
    "True. Using 3x3 depthwise convolution kernels in MobileNet as opposed to regular 3x3 convolutional kernels help reduces the MACs and thus boosts the computational speed by 9x.\n",
    "\n",
    "**Problem 1.9**\n",
    "\n",
    "True. SqueezeNet uses a combination of squeeze modules and expansion modules to preserve larger feature maps at the start of the network and keep computational costs lower until the end.\n",
    "\n",
    "**Problem 1.10**\n",
    "\n",
    "True. Shortcuts in ResNet address the vanishing gradient problem and makes the loss surface smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
